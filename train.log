/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias']
- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1643: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.
  warnings.warn(
Traceback (most recent call last):
  File "training_script.py", line 158, in <module>
    main(data_input_dir, data_output_dir)
  File "training_script.py", line 142, in main
    trainer = Trainer(
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/transformers/trainer.py", line 550, in __init__
    self.init_git_repo(at_init=True)
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/transformers/trainer.py", line 3497, in init_git_repo
    self.repo = Repository(self.args.output_dir, clone_from=repo_name, token=self.args.hub_token)
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/huggingface_hub/repository.py", line 516, in __init__
    self.clone_from(repo_url=clone_from)
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
  File "/home/data1/work/PSST_Training/env/lib/python3.8/site-packages/huggingface_hub/repository.py", line 680, in clone_from
    raise EnvironmentError(
OSError: Tried to clone a repository in a non-empty folder that isn't a git repository ('/home/data1/work/PSST_Training/pth_repo_name'). If you really want to do this, do it manually:
 cd /home/data1/work/PSST_Training/pth_repo_name && git init && git remote add origin && git pull origin main
 or clone repo to a new folder and move your existing files there afterwards.
